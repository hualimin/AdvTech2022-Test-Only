Resilient Multi-Agent Reinforcement Learning with Adversarial Value Decomposition

Related Work
Methods
Experiments
Results and Discussion

基于对抗价值分解的弹性多智能体强化学习

多智能体系统与强化学习
分布式系统由多个独立的组件组成，它们协作完成一个共同的任务。分布式自治系统可以表述为协作多智能体系统(MAS)，该系统可通过强化学习(RL)方法实现
多代理RL
与单代理RL相比，具有更好的可伸缩性和抗变化代理的弹性。我们将代理更改定义为更新或失败。例如，由于维护的原因，一些代理可能会被新软件更新或暂时被其他版本替换。在这两种情况下，我们期望剩下的MAS与这些新代理合作。另一方面，代理可能由于硬件或软件故障而行为错误。在这种情况下，我们希望剩余的MAS能够优雅地降级，而不会完全失效。

现存问题：
大多数最先进的合作MARL方法都专注于优化理想场景，在这种情况下，代理只面对与训练期间相同或类似的代理。这有过拟合的风险，当一些代理显著改变其行为时，MAS可能完全失败，这在安全关键环境中可能是致命的，在这种失败可能会产生灾难性的后果
基于对抗学习的弹性MARL研究，专注于具有固定数量对手agent的专门设置。缺乏所需的灵活性，它们引入了新的可调超参数，如对手的比例或对抗行为的程度，这进一步增加了对意外情况的敏感性。

本文中，提出了
Resilient Adversarial value Decomposition with Antagonist-Ratios（弹性对抗值分解与拮抗剂比） (RADAR)
RADAR提供了一个价值分解方案来训练不同规模的竞争团队，以提高对任意代理变化的弹性。
主要贡献：
1.在训练过程中训练具有可变团队规模的对抗代理的简单机制，且因为RADAR没有引入任何新的超参数，可以很容易地集成到现有的RL框架中
2.一种代理测试方案，以公平的方式持续评估在合作的MAS中对不断变化的代理的性能和弹性
总结：1.RADAR不引入任何新的超参数容易集成。
2.提出了一种代理测试方案，以公平的方式持续评估协同MAS中对变化代理的性能和弹性。
在合作环境中与最先进的MARL竞争，但在测试时面对数量不定的未知对手代理时，RADAR获得了更好的最坏情况性能。
问题公式化
弄成一张简单的映射图？

涉及的的相关概念：
策略梯度强化学习  优势函数  状态价值函数
独立学习 (IAC)、同时学习
集中训练分散执行(CTDE,也即中心化训练去中心化执行)
对于许多问题，培训通常在实验室或模拟环境中进行，在那里可以获得全局信息。最新的MARL利用这一事实来近似集中值函数^Qi，它以全局状态St和联合行动At为条件，并将它们作为公式1中的critic(Lowe等，2017)。而^Qi只需要在训练中学习当地的策略，^πi本身只需要当地历史τt,i为条件，因此它可以以分布式的方式执行。
对抗强化学习

相关工作
对抗强化学习
对抗学习是一种流行的范式，它交替训练两个对手，以提高彼此的性能和鲁棒性
在(单代理)RL中，环境可以被建模为对手，通过添加干扰来对抗最坏情况下的原始代理(Morimoto和Doya 2001;Rajeswaran等2017;Pinto等人2017)。这些对抗性干扰可以通过RL或共同进化方法实现(Gabor等，2019;Wang et al 2019)。
我们的工作主要基于对抗性学习。与单agent RL(外部变化只能发生在环境内部)相比，我们关注的是协同MAS中的agent变化。为此，我们将对手agent整合到训练过程中，以提高应变能力。
对抗性RL方法试图通过对每个代理应用标准RL技术来交替优化或重新制定极小极大目标，从而逼近π_i^*
多智能体强化学习
虽然合作MARL在具有挑战性的领域取得了令人印象深刻的结果，但大多数方法都只在训练中遇到的相同或类似的代理上进行了评估。因此，尚不清楚这些方法是否提供了对任意代理更改的弹性，那些在现实世界中是可预期的。
对于弹性MARL已有一些先前的研究:Minimax-Q在(Littman 1994)中被提出，作为零和游戏Q-Learning的一种改编。在保证对最坏情况下的对手收敛到安全策略的同时，如果对手j的(联合)行动空间很大，Minimax-Q将变得难以处理。
ARTS，其中生产代理和对手代理根据固定的对手比例同时进行训练，因为大多数CDTE方法需要一个预定义的输入维度来近似Q≈Qπ。通过适当选择对手比率，ARTS可以提高对抗代理失败的弹性。然而，一个理想的比率需要先验的知道，这是一个不现实的假设。此外，当N足够大时，固定的比率可以导致敏感的政策。

我们提出了一种对抗价值分解方案，在训练过程中生产代理和对手代理的数量可以动态变化。此外，我们提出了一个代理测试方案 ，以公平的方式评估MARL方法的性能和弹性。

1.基于对抗性学习，将对手agent整合到训练过程中，以提高应变能力。
2.提出了一种对抗价值分解方案，在训练过程中生产代理和对手代理的数量可以动态变化(基于ARTS改进)。
3.提出了一个代理测试方案，以公平的方式评估MARL方法的性能和弹性。
N = 4时的RAT和RADAR方案。左:每个代理都有一个主角(蓝色)和一个对手(红色)代表。中:混合博弈M是随机抽样每个阶段的Rant生成的。右:fi在使用VDN的时代中更新，无论是主角还是对手。注意主角和对手的数量可以根据R的不同而变化。

方法
混合(合作-竞争)博弈Mmix
定义了拮抗剂(对手)比例???
.........+++

Dpro是一个由生产代理或主角代理组成的团队，它们需要完成某个(合作)任务。Dant是对手代理，代表MAS(敌对)代理的变化。对于所有主角i∈Dpro和所有对手j∈Dant，对应的奖励定义为rt,i = rt,pro =−rt,j。主角收益(return)Gt,pro的计算方法类似于个人收益Gt,i，使用rt,pro作为奖励，使用γ作为贴现因子。
此外，我们定义了拮抗剂(对手)比例 如果Rant = 0，则Mmix与D = Dpro和一个全局奖励rt,i = rt,pro全力配合对于所有agent i∈ D

大多数针对弹性MARL的方法都专注于具有固定Rant的特定故障场景，它有几个缺点:首先，Rant必须是先验的或广泛调优的，这通常是不可行的。其次，在面对数量变化的代理时，训练期间固定的Rant会导致不灵活的行为，这在现实场景中是可以预期的。第三，Rant会对训练质量本身产生巨大的影响，例如，如果Rant太大，MAS问题就会变得很难学习任何有意义的政策。
因此，我们考虑使用随机对抗训练(RAT)方案。由于在MAS中可以发生任意的代理更改，我们为每个代理提供一个主角和对手表示。


由于在MAS中可以发生任意的代理更改，我们为每个代理提供一个主角和对手表示。我们维护一个池 的主角和一个池 的对手表示，它们和在T阶段训练类似(Pinto et al 2017)。在每个阶段x，我们从均匀分布U中随机抽样Rant∈[0,1)，以运行Ne时期不同的混合博弈My,mix，其中[(1−Rant)N]主体策略πi,pro代表Dy,pro和[RantN]对抗策略πj,ant代表Dy,ant 伴随i!=j的随机选取。每一次y的阶段都可以看作是Dy,pro和Dy,ant之间的零和博弈。在每个阶段之后，fpro或fant会根据生成的经验ey,t= {(hst, zt, at, rt,proi)}交替更新，而另一个池则保持固定。
RAT的完整公式在算法1中给出，其中D为原MAS的agent集合(给定Rant = 0)， N = |D|为agent数量，fpro和fant分别为可学习的主角和对手表示，Ψ为最优化或MARL算法。

弹性对抗价值分解
在RAT中使用独立学习会带来非平稳性的问题，以及对(主角和对手)代理团队缺乏credit分配。大多数CTDE方法都需要一个固定的团队规模，因为^Q≈Qπ的预定义输入维度取决于st和at的联合作用。
因此，我们提出基于VDN的具有可变Rant的主敌策略近似CTDE方案。
虽然我们关注的是混合对策My,mix，但显然Qπ只能近似于合作主体。因此，我们分别使用独立的VDN实例为主角和对手近似Qpro和Qant。

Eq. 2和3的项可以通过反向传播对Qi、pro和Qi、ant进行端到端训练逼近(Sunehag et al 2018)。Qi,pro和Qi,ant可以用来推导出相应的本地政策，可以通过应用于值的多臂老虎机问题，也可以根据公式1通过策略梯度方法。RADAR的主要部件及其与RAT的集成如图1所示。
VDN在我们的背景下提供了一些优势:对于Eq. 2和3,VDN只是近似一个不受特定代理数量限制的和，因此能够处理Dy,pro和Dy,ant的可变团队规模。由于不同于非线性情况下的线性分解，VDN中针对Rant的归一化返回是直接的。

给定算法1中的RAT，我们可以用以下关系近似主角迭代(第17行)的Qpro:
我们可以用以下术语来近似Qant在拮抗剂时期(算法1的第19行):


RAT和RADAR的讨论
RAT和RADAR都没有引入新的超参数(RAT对Rant使用统一采样，RADAR使用VDN而没有额外的近似或目标)，因此调优复杂度完全取决于底层的RL算法。
假设给定Rant和2N个代理表示(N为主角，N为对抗者)的统一采样， RADAR的预期计算复杂度为O(N) ,因为ERant∼U [Rant] = 0.5。最坏情况的复杂性是O(2N)(当每个pro迭代的Rant = 0，而每个ant迭代的Rant接近1)。因此，RADAR的伸缩(规模变化)与其他CTDE方法相似，由于需要训练额外的拮抗剂，因此有一些开销。


评价领域
我们实现了一个具有N个agent的捕食者-被捕食者(PP)
PP[K,N]包含一个K ×K网格，其中有N个学习捕食者代理和N/2个随机移动的被捕食者代理。每个代理从一个随机的位置开始，可以向北、向南、向西、向东移动，或者什么都不做，并且拥有5 × 5的视野。
当至少一个捕食者i与主捕食者处于同一位置，另一个捕食者j != i在i的视线范围内，记录为κ =<hi, ji>时，捕获猎物时的全局奖励+1。捕获的猎物在随机位置重生。我们定义 为主角主要捕获的归一化数量。


安装gym、pygame、box2d环境+？？


本实验在基于gym改进的PP环境下完成，通过日纳入基于VDN网络对


此命令将创建一个格式名为 output/N-agents_domain-D_adversaryratio-D_M_datetime 的文件夹，其中包含保存在protagonist_model.pth和adversary_model.pth中的训练模型，训练进度数据保存在returns.json中，环境状态数据保存在episode_x.json中。


文章提出了一种弹性MAS对抗值分解方案RADAR。雷达训练不同大小的主角和对手代理的竞争团队，提高对任意代理(agent)更改的弹性(适应性)。

与最先进的MARL如COMA、AC-QMIX和IAC相比，RADAR能够实现具有竞争力的合作性能。我们假设随机集成对抗剂的额外训练会导致一些开销，对于合作测试代理的特殊情况，这会牺牲一些性能。然而，对于包括故障场景在内的任意代理更改，RADAR能够获得优越的最坏情况性能。


